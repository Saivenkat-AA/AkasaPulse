import pandas as pd
import numpy as np
import logging
import boto3
import re
import json
from tqdm import tqdm
from botocore.exceptions import ClientError
import uuid
import time
from datetime import datetime
import os
import tempfile

logger = logging.getLogger(__name__)    
logging.basicConfig(level=logging.INFO)


df = pd.read_excel("C:/Users/mallampati.saivenkat/Downloads/feedback_batch_processing.xlsx")

import os
import json
import uuid
import pandas as pd
from datetime import datetime

def create_local_batch_input_file(df, feedback_column, respondent_id_column, output_dir="./batch_temp"):
    """
    Create batch input file locally with Respondent_ID included
    
    Args:
        df (pd.DataFrame): DataFrame containing feedback data
        feedback_column (str): Column name containing feedback text
        respondent_id_column (str): Column name containing respondent ID
        output_dir (str): Directory to save files
    
    Returns:
        str: Path to the created input file
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Generate unique filename
    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
    job_id = str(uuid.uuid4())[:8]
    input_file = os.path.join(output_dir, f"feedback-input-{timestamp}-{job_id}.jsonl")

    
    batch_requests = []
    
    for index, row in df.iterrows():
        feedback_text = str(row[feedback_column])
        respondent_id = str(row[respondent_id_column])  # Get the actual Respondent_ID
        
        # Create the prompt for each feedback with Respondent_ID included
        user_message = f"""You are expert in analyzing the employee feedback sentiment.

Your task is to classify the sentiment of the given employee feedback into one of the 3 categories: Positive, Neutral or Negative.

Sentiment Analysis Guidelines:
1. Analyze the feedback to determine the overall sentiment.
2. Classify the sentiment as Positive, Neutral or Negative based on the tone and content of the feedback.
3. If the feedback contains both positive and negative sentiments, always prioritize the negative sentiment.
4. Only use the sentiments: Positive, Neutral or Negative and do not create new categories in any case.
5. Explainations or additional text should not be present in the output. Only the sentiment classification should be returned.
6. Ensure the output will always be in the format: Sentiment: [classification]

Present your answer in this exact format:


Respondent_ID : "{respondent_id}"
Customer Feedback: "{feedback_text}"
Sentiment: [classification]


"""
        
        # Create batch request item using Respondent_ID as recordId
        batch_request = {
            "recordId": respondent_id,  # Use actual Respondent_ID instead of index
            "modelInput": {
                "messages": [
                    {
                        "role": "user",
                        "content": [{"text": user_message}]
                    }
                ],
                "inferenceConfig": {
                    "maxTokens": 100,
                    "temperature": 0,
                    "topP": 0.1
                }
            }
        }
        batch_requests.append(batch_request)
    
    # Write to JSONL file
    with open(input_file, 'w', encoding='utf-8') as f:
        for request in batch_requests:
            f.write(json.dumps(request) + '\n')
    
    print(f"Batch input file created: {input_file}")
    print(f"Total requests: {len(batch_requests)}")
    return input_file

def upload_to_s3_temp(file_path, s3_client, bucket_name):
    """
    Upload file to S3 temporarily for batch processing
    
    Args:
        file_path (str): Local file path
        s3_client: S3 client
        bucket_name (str): S3 bucket name
    
    Returns:
        str: S3 URI
    """
    filename = os.path.basename(file_path)
    s3_key = f"temp-batch-inference-sentiment/{filename}"
    
    try:
        s3_client.upload_file(file_path, bucket_name, s3_key)
        s3_uri = f"s3://{bucket_name}/{s3_key}"
        print(f"File uploaded to: {s3_uri}")
        return s3_uri
    except ClientError as e:
        print(f"Error uploading to S3: {e}")
        return None

def cleanup_temp_files(*file_paths):
    """
    Clean up temporary files
    
    Args:
        *file_paths: Variable number of file paths to delete
    """
    for file_path in file_paths:
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
                print(f"Cleaned up: {file_path}")
            except Exception as e:
                print(f"Error cleaning up {file_path}: {e}")

def process_feedback_batch_local(df, feedback_column, s3_bucket, role_arn, aws_credentials, cleanup_files=True): #1
    """
    Process feedback using batch inference with local file handling
    
    Args:
        df (pd.DataFrame): DataFrame with feedback data
        feedback_column (str): Column name containing feedback text
        s3_bucket (str): S3 bucket for temporary files (still needed for batch processing)
        role_arn (str): IAM role ARN for batch processing
        aws_credentials (dict): AWS credentials dictionary
        cleanup_files (bool): Whether to clean up temporary files
    
    Returns:
        pd.DataFrame: DataFrame with categories and sentiments added
    """
    
    # Initialize AWS clients
    bedrock_client = boto3.client(
        "bedrock",
        aws_access_key_id=aws_credentials['aws_access_key_id'],
        aws_secret_access_key=aws_credentials['aws_secret_access_key'],
        aws_session_token=aws_credentials['aws_session_token'],
        region_name=aws_credentials['region_name']
    )
    
    s3_client = boto3.client(
        "s3",
        aws_access_key_id=aws_credentials['aws_access_key_id'],
        aws_secret_access_key=aws_credentials['aws_secret_access_key'],
        aws_session_token=aws_credentials['aws_session_token'],
        region_name=aws_credentials['region_name']
    )
    
    model_id = "apac.amazon.nova-pro-v1:0"
    
    # Generate unique identifiers
    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
    job_id = str(uuid.uuid4())[:8]
    
    local_input_file = None
    local_output_file = None
    
    try:
        # Step 1: Create local input file
        print("Step 1: Creating local input file...")
        local_input_file = create_local_batch_input_file(df,respondent_id_column='Respondent_ID', feedback_column = feedback_column)
        
        # Step 2: Upload to S3 temporarily
        print("Step 2: Uploading to S3 temporarily...")
        input_s3_uri = upload_to_s3_temp(local_input_file, s3_client, s3_bucket)
        if not input_s3_uri:
            return df
        
    
    finally:
        # Cleanup temporary files
        if cleanup_files:
            cleanup_temp_files(local_input_file, local_output_file)
    
def process_feedbacks(df, feedback_column, s3_bucket, role_arn):
    """
    Simple function to process feedbacks with your existing credentials
    
    Args:
        df (pd.DataFrame): DataFrame with feedback data
        feedback_column (str): Column name containing feedback text
        s3_bucket (str): S3 bucket name
        role_arn (str): IAM role ARN
    
    Returns:
        pd.DataFrame: DataFrame with results
    """
    aws_credentials = {
        'aws_access_key_id': 'A',
        'aws_secret_access_key': 'Um',
        'aws_session_token': 'IQoJb3J',
        'region_name': 'ap-south-1'
    }
    
    return process_feedback_batch_local(df, feedback_column, s3_bucket, role_arn, aws_credentials)

result_df = process_feedbacks(df=df,feedback_column='Feedback',s3_bucket='akasa-bedrock',role_arn='arn:aws:iam::891377165721:role/Amazon-Bedrock-Batchinference-Role')
